{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "**This Jupiter Notebook created for the purposes of setting up and running Ai models with [Verbum OCR](https://github.com/ragelmalti/verbum_ocr).**\n",
        "\n",
        "VerbumOCR is capable of running various Ai models to perform OCR. It utilises the OpenAI Python library, to make API requests to any Ai model that utilises the OpenAI API specification.\n",
        "\n",
        "This notebook will be split into two parts:\n",
        "1. Instructions for setting up VerbumOCR with **popular properitary models**, including ChatGPT and Google Gemini\n",
        "2. Instructions and code for hosting **locally hosted models from HuggingFace** using either **vLLM or Ollama.**\n",
        "  - **Note:** It's suggested if you run the code in this Jupiter Notebook, that you utilise Google Colab's Nvidia A100 40GB GPU for the best results. Otherwise, a high performance GPU on a local machine will substitute.\n",
        "\n",
        "**IMPORTANT:** A `config.env` file needs to be created in the base directory where the `verbum_ocr.py` script will be run.\n",
        "\n",
        "You'll need to add/change the `LLM_BASE_URL` and `LLM_API_KEY` env variables depending on what model you're using."
      ],
      "metadata": {
        "id": "bhP-DV044-Mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #1 Setting up properitary models\n",
        "## Google Gemini\n",
        "*Instructions adapted from: https://ai.google.dev/gemini-api/docs/openai*\n",
        "\n",
        "*   Generate an API key with [Google Ai Studio](https://aistudio.google.com/apikey).\n",
        "*   Set the `LLM_API_KEY` env variable in `config.env` to include the API key generated prior.\n",
        "*   Set the `LLM_BASE_URL` env variable to: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "*   When running the `verbum_ocr.py` script, set the `--model_name` flag to include a Gemini model, e.g. `gemini-pro-vision` or `gemini-2.5-flash`\n",
        "\n",
        "\n",
        "## ChatGPT\n",
        "**NOTE:** Make sure you add credits via the OpenAPI Platform portal: https://platform.openai.com/settings/organization/billing/overview\n",
        "\n",
        "*Instructions adapted from: https://platform.openai.com/docs/api-reference/responses/create*\n",
        "*   Generate an API key with [OpenAI Platform](https://platform.openai.com/api-keys)\n",
        "*   Set the `LLM_API_KEY` env variable in `config.env` to include the API key generated prior.\n",
        "*   Set the `LLM_BASE_URL` env variable to: \"https://api.openai.com/v1/responses\"\n",
        "*   When running the `verbum_ocr.py` script, set the `--model_name` flag to include a ChatGPT model, e.g. `gpt-5-nano`\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zpPCdgI7pNKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #2 Setting up local hosted models from HuggingFace"
      ],
      "metadata": {
        "id": "3RpFyQ2_pfPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python PIP Dependencies for vLLM and Verbum OCR"
      ],
      "metadata": {
        "id": "q3FCqte1OL_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FQP7fquQrBvC"
      },
      "outputs": [],
      "source": [
        "# vLLM Dependencies\n",
        "!pip install openai>=1.52.2\n",
        "!pip install vllm>=0.6.3\n",
        "!pip install triton>=3.1.0\n",
        "!pip install nest_asyncio # only needed in colab\n",
        "# Verbum OCR Dependencies\n",
        "!pip install jiwer\n",
        "!pip install pymupdf\n",
        "!pip install requests\n",
        "!pip install python-dotenv\n",
        "!pip install google-genai\n",
        "# Ngrok dependecy\n",
        "!pip install pyngrok\n",
        "!pip check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This should not be necessary outside of colab.\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "Oe0AQb2rrLw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Ollama\n",
        "Adapted from https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb"
      ],
      "metadata": {
        "id": "DqCA8SiQIPZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "YlVK9iG4AD5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running via Ngrok\n",
        "\n",
        "Ngrok will be used to as a way to \"port foward\" the Ollama or vLLM instance to the internet.\n",
        "\n",
        "The pyngrok library is used: https://pyngrok.readthedocs.io/en/latest/integrations.html#colab-ssh-example\n",
        "\n",
        "\n",
        "Set the `LLM_BASE_URL` var in `config.env` to the ngrok tunnel URL, with the endpoint, /v1 at the end.\n",
        "\n",
        "Set `LLM_API_KEY=123`\n",
        "\n",
        "E.g. `LLM_BASE_URL=https://49c194b286d2.ngrok-free.app/v1`"
      ],
      "metadata": {
        "id": "KGhVGv01s1sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kMZollI20-S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running on local machine\n",
        "\n",
        "If the ai model is running on your local machine, set the `LLM_BASE_URL` var in `config.env` to `LLM_BASE_URL=http://127.0.0.1/8000/v1`\n",
        "\n",
        "Set `LLM_API_KEY=123`"
      ],
      "metadata": {
        "id": "NM0kQaE305Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute vLLM\n",
        "Link to the vLLM instructions for Google Colab: https://cloud.google.com/dataflow/docs/notebooks/run_inference_vllm"
      ],
      "metadata": {
        "id": "c9c4loSFODNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision models tested with vLLM\n",
        "Models are stored in `/root/.cache/huggingface/hub`\n",
        "\n",
        "* ✅ Qwen/Qwen2.5-VL-3B-Instruct\n",
        "* ✅ nanonets/Nanonets-OCR-s\n",
        "* ✅ llava-hf/llava-v1.6-mistral-7b-hf\n",
        "* ✅ ibm-granite/granite-vision-3.2-2b\n",
        "* ✅ allenai/olmOCR-7B-0225-preview\n",
        "* ✅ ChatDOC/OCRFlux-3B\n",
        "* ✅ reducto/RolmOCR\n",
        "* ? rednote-hilab/dots.ocr\n",
        "* ❌ deepseek-ai/deepseek-vl2-tiny\n",
        "* ❌ google/gemma-3-4b-it\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I5w_BtBSw6sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from pyngrok import conf\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# = NGROK SETUP =\n",
        "# Auth token copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "os.environ[\"NGROK\"] = userdata.get(\"NGROK\")\n",
        "conf.get_default().auth_token = os.environ[\"NGROK\"]\n",
        "port = 8000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "# = START vLLM =\n",
        "\n",
        "!python -m vllm.entrypoints.openai.api_server --trust-remote-code --model reducto/RolmOCR"
      ],
      "metadata": {
        "id": "W7zdoR0BrOnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Ollama\n",
        "Check https://ollama.com/search?c=vision for a list of vision models"
      ],
      "metadata": {
        "id": "y4jRBgMaFahX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from pyngrok import conf\n",
        "from google.colab import userdata\n",
        "\n",
        "# = NGROK SETUP =\n",
        "# Auth token copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "os.environ[\"NGROK\"] = userdata.get(\"NGROK\")\n",
        "conf.get_default().auth_token = os.environ[\"NGROK\"]\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "# = START OLLAMA =\n",
        "\n",
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "o2ghppmRDFny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}