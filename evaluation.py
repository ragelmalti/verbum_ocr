# Script used to compare the similarity between the ground truth, and LLM output
# Both outputs will be in markdown format

import jiwer
import argparse
import json
import difflib
from difflib import SequenceMatcher
from markdown_it import MarkdownIt
from collections import defaultdict

# https://github.com/rtfpessoa/diff2html?tab=readme-ov-file
# Add an argument that defines diff file output
parser = argparse.ArgumentParser()
parser.add_argument("--ground_truth", help="Markdown file containing the ground truth", type=str, required=True)
parser.add_argument("ocr_output", help="JSON file containing the OCR output generated by the LLM, and other information")
args = parser.parse_args()
md = MarkdownIt()

def calculate_cer_and_wer(ground_truth, ocr_output):
    cer = jiwer.cer(ground_truth, ocr_output)
    wer = jiwer.wer(ground_truth, ocr_output)
    return (cer, wer)

def generate_unified_diff(ground_truth, ocr_output):
    unified_diff = difflib.unified_diff(ground_truth.splitlines(), ocr_output.splitlines(), lineterm='')
    with open("unified_diff", "w", encoding="utf-8") as f:
        f.write('\n'.join(list(unified_diff)))

def read_json_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)
        return data

def get_test_data_statistics(json):
    prompt_token_count = 0
    completion_token_count = 0
    total_tokens = 0
    markdown = ""
    llm_model = json[0]["llm_model"]
    for item in json:
        if item["token_count"]:
            prompt_token_count += item["token_count"]["prompt_tokens"]
            completion_token_count += item["token_count"]["completion_tokens"]
        markdown += item["markdown"] + "\n"
    
    total_tokens = prompt_token_count + completion_token_count

    return (prompt_token_count, completion_token_count, total_tokens, markdown)

def read_markdown_file(file_path):
    # Read markdown file, and return it as a str variable
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.read()
        return data

def extract_markdown_from_JSON(data):
    all_markdown = "\n".join(item["markdown"] for item in data)
    return all_markdown

def extract_features(markdown_text):
    md = MarkdownIt()
    tokens = md.parse(markdown_text)
    features = []

    i = 0
    while i < len(tokens):
        token = tokens[i]

        if token.type == "heading_open":
            content = tokens[i+1].children[0].content if tokens[i+1].children else ""
            features.append({"type": token.tag, "content": content})
            i += 2 
        elif token.type == "bullet_list_open":
            items = []
            j = i + 1
            while tokens[j].type != "bullet_list_close":
                if tokens[j].type == "list_item_open":
                    inline = tokens[j+2]  
                    text = "".join(child.content for child in inline.children if child.type == "text")
                    items.append(text)
                    j += 4 
                else:
                    j += 1
            features.append({"type": "ul", "content": items})
            i = j + 1
        elif token.type == "fence":  # code block
            features.append({"type": "code_block", "content": token.content})
            i += 1
        elif token.type == "code_inline":
            features.append({"type": "code_inline", "content": token.content})
            i += 1
        else:
            i += 1

    return features

def similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()

def compare_features(gt_features, ocr_features):
    matches = []
    missing = []

    gt_used = [False] * len(gt_features)

    for ocr in ocr_features:
        match_found = False
        for idx, gt in enumerate(gt_features):
            if gt_used[idx]:
                continue
            
            if gt["type"] == ocr["type"]:
                similiarity = similarity(gt["content"].lower(), ocr["content"].lower())
                if similiarity >= 0.8:
                    matches.append((gt, ocr))
                    gt_used[idx] = True
                    break

    # Any GT feature not used is considered missing in OCR
    for idx, used in enumerate(gt_used):
        if not used:
            missing.append(gt_features[idx])

    return {
        "num_gt_features": len(gt_features),
        "num_ocr_features": len(ocr_features),
        "matches": matches,
        "missing": missing
    }

def main():
    json_input = read_json_file(args.ocr_output)
    ground_truth = read_markdown_file(args.ground_truth)
    # return (prompt_token_count, completion_token_count, total_tokens, markdown)
    prompt_token_count, completion_token_count, total_tokens, ocr_markdown = get_test_data_statistics(json_input)
    CER, WER = calculate_cer_and_wer(ground_truth, ocr_markdown)
    
    #generate_unified_diff(ground_truth, ocr_markdown)

    c = compare_features(extract_features(ground_truth), extract_features(ocr_markdown))
    pretty_json = json.dumps(c, indent=4)
    print(pretty_json)
    print("=== Exact matches to the GT: ===")
    print(f"# of Matches: {len(c["matches"])}")
    print(len(c["matches"]) / c["num_gt_features"])

    print("=== Other Stats: ===")
    print(f"CER: {CER}\nWER: {WER}")
    print(f"Total Input Tokens: {prompt_token_count}")
    print(f"Total Output Tokens: {completion_token_count}")
    print(f"Total Combined Tokens: {total_tokens}")


if __name__ == "__main__":
    main() 